{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7de238",
   "metadata": {},
   "source": [
    "Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45fce7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY not set. Please add it to your .env.\")\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "if TAVILY_API_KEY:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0c1cc",
   "metadata": {},
   "source": [
    "LLM & basic RAG components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993415cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Groq LLM (choose any supported model)\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Simple in-memory corpus for demo;\n",
    "# replace with loaders for your real docs\n",
    "raw_docs = [\n",
    "    Document(\n",
    "        page_content=\"Agentic RAG combines retrieval-augmented generation with tool-using agents \"\n",
    "        \"that can call web search, query vector stores, and iteratively reason.\",\n",
    "        metadata={\"source\": \"internal_notes.md\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Tavily is a search API optimized for LLM agents, providing curated web results \"\n",
    "        \"with citations and metadata.\",\n",
    "        metadata={\"source\": \"tavily_overview.md\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "# Embeddings + FAISS vectorstore\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab48f9",
   "metadata": {},
   "source": [
    "Plain RAG chain with source docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d9b5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert assistant. Use ONLY the provided context to answer. \"\n",
    "            \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "            \"When possible, cite sources by name from the metadata.\\n\\n\"\n",
    "            \"Context:\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": lambda x: x[\"question\"]}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def rag_answer(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simple RAG pipeline: retrieve docs from the vector store and answer using them.\n",
    "    Returns both the answer and the underlying source documents for the UI.\n",
    "    \"\"\"\n",
    "    source_docs = retriever.get_relevant_documents(question)\n",
    "    context_text = \"\\n\\n\".join([d.page_content for d in source_docs])\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": question, \"context\": context_text})\n",
    "\n",
    "    sources = []\n",
    "    for d in source_docs:\n",
    "        sources.append(\n",
    "            {\n",
    "                \"title\": d.metadata.get(\"source\", \"unknown\"),\n",
    "                \"snippet\": d.page_content[:400] + (\"...\" if len(d.page_content) > 400 else \"\"),\n",
    "                \"url\": d.metadata.get(\"url\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\"answer\": answer, \"sources\": sources}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2858b1",
   "metadata": {},
   "source": [
    "Tools: Tavily search + RAG as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdb15b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tavily web search tool\n",
    "if not TAVILY_API_KEY:\n",
    "    print(\"Warning: TAVILY_API_KEY not set. Web search tool will still be created but may fail.\")\n",
    "\n",
    "tavily_tool = TavilySearchResults(\n",
    "    max_results=4,\n",
    "    # Use environment variable TAVILY_API_KEY automatically\n",
    ")\n",
    "\n",
    "def rag_tool_fn(query: str) -> str:\n",
    "    \"\"\"Thin wrapper around rag_answer for use as an agent Tool.\"\"\"\n",
    "    result = rag_answer(query)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"internal_rag_qa\",\n",
    "    description=(\n",
    "        \"Use this tool to answer questions about the internal knowledge base. \"\n",
    "        \"It performs retrieval-augmented generation over embedded documents.\"\n",
    "    ),\n",
    "    func=rag_tool_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b4b3d",
   "metadata": {},
   "source": [
    "Tool-using agent (LangChain agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769ab56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    rag_tool,\n",
    "    tavily_tool,\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db9a56",
   "metadata": {},
   "source": [
    "Unified API for Streamlit: run_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38265ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    query: str,\n",
    "    prefer_agent: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Unified entrypoint for the Streamlit app.\n",
    "\n",
    "    - If prefer_agent=True, route through the tool-using agent (rag tool + Tavily search).\n",
    "    - In all cases, we also independently run the RAG pipeline to surface concrete sources\n",
    "      for the UI, so you always have some citations.\n",
    "    \"\"\"\n",
    "    # Always gather RAG-based sources\n",
    "    rag_result = rag_answer(query)\n",
    "\n",
    "    if prefer_agent:\n",
    "        try:\n",
    "            agent_answer = agent.run(query)\n",
    "        except Exception as e:\n",
    "            agent_answer = (\n",
    "                f\"[Agent failed, falling back to pure RAG]: {e}\\n\\n\"\n",
    "                f\"{rag_result['answer']}\"\n",
    "            )\n",
    "    else:\n",
    "        agent_answer = rag_result[\"answer\"]\n",
    "\n",
    "    return {\n",
    "        \"answer\": agent_answer,\n",
    "        \"sources\": rag_result[\"sources\"],\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
